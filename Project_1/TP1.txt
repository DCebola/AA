Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardize the attribute values.
R1: The classification tasks are performed over multivariate data. Due to the magnitude of the values being very different for each of the features (for example, in line23 of the train data,
the variance is -0.235 and the skewness is -14.740), there is the need to standardize the data to ensure that all features are interpreted with equal weight.

Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: While preprocessing the data the mean and standard deviation of the whole training data is calculated, then both the training and test data are standardise using these parameters.
The distribution of each feature will be translated considering the mean and scaled considering the standard deviation, so that the final value of the mean is 0 and of the standard deviation is 1.

Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: To calculate the prior probability of an example belonging to a class we divide the total number of examples belonging to a a given class by the total number of examples.

Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: To classify a given example, we calculate, for each class, the logarithm of the prior probability and sum it to the sum of the estimated probability for each feature. After the calculations,
the class for which the sum is the biggest is determined to be the example's class. This can be seen implemented in the private classify function of the custom naive bayes.

Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: The bandwidth parameter affects how the kernel function calculates the estimated probability distribution for each feature - the higher the bandwidth, the lower the weight that close feature
values will have on the feature's distribution. The higher the bandwidth the less strict the classifier's estimate will be. High bandwidth can lead to underfitting whereas low bandwidth can lead
to overfitting.

Q6: Explain what effect the C parameter has on the Logistic Regression classifier.
R6: The Logistic Regression used a L2 type regularization so the C parameter corresponds to 1/λ. As C is inversely proportional to λ, high values of C tend to overfit the classifier whereas low values
of C tend to underfit the classifier

Q7: Explain how you determined the best bandwidth and C parameters for your classifier and the Logistic Regression classifier. You may include a relevant piece of your code if this helps you explain.
R7: An iteration over different values was performed for each of the parameters. In each iteration 5-fold cross validation was used to calculate the validation error. After all the iterations were finished
the value that was picked for each parameter was the one where the respective average validation error was the lowest.

Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: Each classifier, using the respective optimized parameter, was fitted with all the training data. Using the features from the test data, the final hypothesis were obtained for all the classifiers.

Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9: Results as obtained from a random run of the script:
----------------------CrossValidation----------------------
Best C: 100
Best bandwidth: 0.08
--------------------------Scores---------------------------
Logistic Regression errors: 134.00
Custom gaussian naive bayes errors: 143.00
Gaussian naive bayes errors: 163.00
----------------Approximate Normal Test (95%)--------------
Logistic Regression: 134.00 ± 21.44
Custom gaussian naive bayes: 143.00 ± 22.06
Gaussian naive bayes: 163.00 ± 23.34
----------------------McNemar's Test-----------------------
Logistic Regression vs Custom gaussian naive bayes: 0.85
Logistic Regression vs Gaussian naive bayes: 12.44
Custom gaussian naive bayes vs Gaussian naive bayes: 6.45
-----------------------------------------------------------
From the values obtained, analysing the approximate normal test the Logistic Regression and Custom Gaussian Naive Bayes tend to perform, in terms of total errors, similarly, with the
Gaussian Naive Bayes provided by the library performing worse than the prior. Further inspecting the Mc Nemar's Test results it can be concluded that:
 - Logistic Regression and Custom Gaussian Naive Bayes perform identically (score lower than 3.84);
 - Logistic Regression and Gaussian Naive Bayes from the library do not perform identically (score higher than 3.84);
 - Custom Gaussian Naive Bayes and Gaussian Naive Bayes from the library may perform identically or not (the score tends oscillate and most of the times is higher than 3.84
 but sometimes can go as low as , observed on runs where the best bandwidth found was over 0.1) - this can be explained from the randomness of the splits when cross validating to find the
 best value of bandwidth;


