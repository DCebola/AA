Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.
- You can include references to images or html files such as the reports generated with clusters. To do this, simply include this document in the folder with the reports or images and refer them in the text by the file name in an isolated line. For example, the line

test.png

refers to a test.png image file in the same folder as this document.

QUESTIONS:

Q1: Explain how you selected the best attributes for the clustering phase. In particular, indicate the visualization methods used to explore the extracted attributes and any statistical tests used.
R1: To visualize the correlation between features and spot redundant features we used correlation heatmaps. When feature selecting we plotted several metrics obtained from the clusters
as a function of the used features - recall, precision, adjusted Rand index, Adjusted Rand Index f1 measure and Rand index.
To select the best features we performed sequential backwards elimination. On each iteration of the sequential backwards elimination,
the features were sorted by their f-score - this f-score was based on the labels generated in the iteration.
We also automated the choice of the best combination of features by using an heuristic.
This heuristic was the weighted sum of the Adjusted Rand Index value, F1-Measure and adjusted Rand index.
After the sequential backwards elimination we also removed features that were too correlated, keeping only one of the set for each set of correlated features found.

Q2: After selecting the attributes, did you standardize or normalize the values? Justify your decision.
R2: We observed that the features extracted using PCA and Isomap had a much higher scale compared to the features extracted using t-SNE. Based on these observations and knowing that both the K-Means
and the estimation of the best epsilon for the DBSCAN used the euclidean distance, normalizing the features seemed the best approach to have each feature contributing with the same weight. This yielded
better overall for the K-Means, and worse for the DBSCAN algorithm.
After observing the variance of each feature and seeing their different scales we also tried standardizing the data.
For the K-Means, the results were similar to the ones seen when using features with their original values, we explain this as a result of the difference in the scale of variance being useful
for the estimation of the clusters in these algorithms. For the DBSCAN the results were the best found due to the standardization maintaining the original density of the distribution of points.
After observing the clusters found in each experiment for each algorithm, we concluded that we should normalize the values of the features - although the results for DBSCAN were worse, even
the best results for this algorithm produced unuseful clusters. (We did all the observations in experiments where we did not apply any feature selection.)

Q3: Explain how you found the neighborhood radius value (epsilon) for the DBSCAN algorithm by following the procedure described in the article "A density-based algorithm for discovering clusters in large spatial databases with noise".
R3: We calculated the distance matrix for the 5th nearest neighbour for each point and sorted it in descending order.
Then we found the epsilon by estimating the first "valley" of this function (found by scipy.signal.find_peaks_cwt when applied to the inverse of the sorted distance function)

epsilon.png

Q4: Examining the clusters generated by the DBSCAN algorithm with the value optimized by the method described in the article, do you think the result is adequate to cluster these images? Justify your answer.
R4: The algorithm only found one cluster with the estimated epsilon using the method described in the article. The biologists needed 3 different clusters, so the DBSCAN algorithm is not useful with this method of estimating the epsilon.
(This experiment was run with feature selection and feature normalization.)

Q5: Describe your analysis of the k (for K-Means) and epsilon (for DBSCAN) parameters using the internal and external indicators referred in the assignment page. Include the two plots of the indicator values (indicating the image name of each plot in one line in your answer) as a function of the k and epsilon parameters and explain how you chose the ranges for examining these parameters. Indicate, with justification, what conclusions you can draw from this analysis.
R5: We did the analysis in experiments where the features were not normalized and where no features selection was applied.
For the K-Means algorithm we changed the range from 2 to n clusters and for the DBSCAN we change the epsilon value from 0 to best epsilon in k = best epsilon / (n - 1) steps.
Considering an experiment with n = 10, we conclude that the best number of clusters is 3 because, although the silhouette score is not the highest the F1-Measure maximizes and the Adjusted Rand Index stabilizes at 3 clusters.
Considering the same experiment, we found that for obtaining similar metrics to K-Means in terms of F1-Measure, Adjusted Rand Index and Silhouette Score the best epsilon is not the one we are estimating but the one 1 step before.
This means that we are estimating a wrong epsilon, which makes us conclude that only considering the first valley is not adequate.

dbscan_parameter_metrics.png
kmeans_parameter_metrics.png

Q6: Select some of the parameter values tested in question five and examine the corresponding clusters more closely, generating the HTML file with the images. Explain how you selected these parameter values, discuss the different options and propose a recommendation that could help the biologists' task of classifying cells and rejecting segmentation errors.
R6: We implicitly choose the Silhouette Score, the F1-Measure and Adjusted Rand Index with the heuristic (respectively with weights of 1,2,1) used while finding the best set of features for each algorithm - this heuristic can be manipulated to consider other metrics
and with different weights for each metric. The clusters evaluated will be the ones generated for the best set of features for each algorithm, using normalized features.
DBSCAN only generated one cluster while K-Means generated 3. The clusters generated by the K-Means seemed to separate the cells based on the average pixel value of each image which is not particularly useful for clustering based on shape.
At most, biologists can manipulate the heuristic to try and tweak the desired final cluster metrics. We recommend that the biologist first try and apply a filter to the images with, for example, a gaussian kernel mask of 50x50pixels, so that the interior of the cells would have the most
influence in the information extracted for the features. Only min-maxing parameters and feature values without proper selection of data to fit the algorithms might not be the best approach.

kmeans_clusters.png
kmeans_clusters.html
dbscan_clusters.png
dbscan_clusters.html

Q7: Discuss advantages or problems with these two algorithms (K-Means and DBSCAN) for the purpose of helping biologists to organize these images, considering your theoretical knowledge of these algorithms as well as the results you obtained in your work.
R7: DBSCAN finds clusters from regions with high density if the regions are not well defined and separated it will struggle to create more than a few clusters (in this case it only creates one). On the other hand,
K-Means is not influenced by regions not being well defined or overlapping, it assumes clusters of even size and so, even if the clusters are not correct, it is capable of creating n clusters.
From the results obtain we recommend the K-Means algorithm over the DBSCAN algorithm because it is possible to at least control the number of clusters to generate, even if they are not the most adequate.

Q8: Consider other clustering algorithms embedded in the Scikit-Learn library. Choose one and apply it to this problem, optimizing the parameters you deem appropriate in the way that you find adequate. Justify your choices and discuss whether this option would yield more useful results for biologists.
R8: We chose the Spectral Clustering algorithm because we could still control the number of clusters to generate and it was adequate for the number of samples/clusters.
It uses a distance graph which means normalizing the features would benefit its performance just like in K-Means.
We fixed the number of clusters to generate to 3 and tried to experiment with different values for the number of K-Means iterations and number of Neighbours to each point.
We observed that varying these parameters did not yield any different results, so we decided to not vary any parameter and just keep them with the default value.
Overall, it performed similarly with the K-Means algorithm, so it is as useful as the K-Means algorithm for the biologists.

Not satisfied with these results, we tried to apply the previous suggestion that we gave in question 6. The results were quite fascinating and inline with what we expected.
Applying the filter yielded better results even without feature selection. In fact, feature selection actually created worse clusters meaning it could be "overfitting" the algorithm to the labels given by the biologists.
The best results were found using spectral clustering without feature selection over normalized features. In these clusters we could clearly identify the three phases with little error.

spectral_clusters.html
kmeans_clusters.html
dbscan_clusters.html
spectral_clusters_with_kernel.html
kmeans_clusters_with_kernel.html
dbscan_clusters_with_kernel.html
kernel_filtered_images.zip (example of some images filtered with a kernel we chose)

Q9: (Optional) Implement the Bissecting K-Means hierarchical clustering algorithm as described in the assignment page and Lecture 19. Examine and discuss the results and their application to the problem of helping the biologists select and classify cell images.
R9: After implementation and testing the Bissecting K-Means hierarchical clustering algorithm we found that it yielded similar results to the ones provided by the K-Means algorithm.
It would be useful if, for the same phase of cell division, wanted to have sub-categories.

bisecting_kmeans_cluster.html
bisecting_kmeans_clusters_with_kernel.html